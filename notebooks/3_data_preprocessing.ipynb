{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data Preprocessing Steps\n",
    "1. Remove Informational Offers\n",
    "2. Remove unneeded features\n",
    "3. Split data into Train/Test/Validation\n",
    "4. Use Sklearn Pipeline to Preprocess Data\n",
    "5. Upload Files to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# This is an object that represents the SageMaker session that we are currently operating in. This\n",
    "# object contains some useful information that we will need to access later such as our region.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# This is an object that represents the IAM role that we are currently assigned. When we construct\n",
    "# and launch the training job later we will need to tell it what IAM role it should have. Since our\n",
    "# use case is relatively simple we will simply assign the training job the role we currently have.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 63288 entries, 0 to 63287\n",
      "Data columns (total 28 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   person                 63288 non-null  object        \n",
      " 1   event                  63288 non-null  object        \n",
      " 2   time_received          63288 non-null  int64         \n",
      " 3   amount                 63288 non-null  float64       \n",
      " 4   offer_id               63288 non-null  object        \n",
      " 5   reward                 63288 non-null  float64       \n",
      " 6   difficulty             63288 non-null  float64       \n",
      " 7   duration               63288 non-null  float64       \n",
      " 8   offer_type             63288 non-null  object        \n",
      " 9   email                  63288 non-null  float64       \n",
      " 10  mobile                 63288 non-null  float64       \n",
      " 11  web                    63288 non-null  float64       \n",
      " 12  social                 63288 non-null  float64       \n",
      " 13  total_channels         63288 non-null  float64       \n",
      " 14  gender                 63288 non-null  object        \n",
      " 15  age                    63288 non-null  int64         \n",
      " 16  income                 55222 non-null  float64       \n",
      " 17  member_date            63288 non-null  datetime64[ns]\n",
      " 18  member_year            63288 non-null  int64         \n",
      " 19  tenure                 63288 non-null  int64         \n",
      " 20  offer_received_count   63288 non-null  int64         \n",
      " 21  time_viewed            49135 non-null  float64       \n",
      " 22  offer_viewed_count     63288 non-null  int64         \n",
      " 23  time_completed         28996 non-null  float64       \n",
      " 24  offer_completed_count  63288 non-null  int64         \n",
      " 25  offer_viewed_flag      63288 non-null  int64         \n",
      " 26  offer_completed_flag   63288 non-null  int64         \n",
      " 27  conversion_flag        63288 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(12), int64(10), object(5)\n",
      "memory usage: 14.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_conversion = pd.read_pickle('../input/df_conversion.pkl')\n",
    "df_conversion.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Remove Informational Offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50637, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conversion_v2 = df_conversion[df_conversion['offer_type'] != 'informational']\n",
    "df_conversion_v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Remove Unneeded Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reward</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>total_channels</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>tenure</th>\n",
       "      <th>offer_viewed_flag</th>\n",
       "      <th>conversion_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>discount</td>\n",
       "      <td>3.0</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>bogo</td>\n",
       "      <td>4.0</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>discount</td>\n",
       "      <td>4.0</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>bogo</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>discount</td>\n",
       "      <td>2.0</td>\n",
       "      <td>O</td>\n",
       "      <td>40.0</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reward  difficulty  duration offer_type  total_channels   gender   age  \\\n",
       "0     2.0        10.0       7.0   discount             3.0        M  33.0   \n",
       "3     5.0         5.0       5.0       bogo             4.0        M  33.0   \n",
       "4     2.0        10.0      10.0   discount             4.0        M  33.0   \n",
       "5     5.0         5.0       5.0       bogo             4.0  Unknown   NaN   \n",
       "6     5.0        20.0      10.0   discount             2.0        O  40.0   \n",
       "\n",
       "    income  tenure  offer_viewed_flag  conversion_flag  \n",
       "0  72000.0       1                  0                0  \n",
       "3  72000.0       1                  1                1  \n",
       "4  72000.0       1                  1                1  \n",
       "5      NaN       0                  1                0  \n",
       "6  57000.0       0                  1                1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['person', 'event', 'time_received', 'amount', 'offer_id', 'email', 'mobile', 'web', 'social', 'member_date', 'member_year'\\\n",
    "                 ,'offer_received_count', 'time_viewed', 'offer_viewed_count', 'time_completed', 'offer_completed_count', 'offer_completed_flag']\n",
    "df = df_conversion_v2.drop(columns_to_drop, axis = 1)\n",
    "\n",
    "# convert values where age is 118 to null\n",
    "df['age'] = df['age'].map(lambda x: np.nan if x == 118 else x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we package up the input data and the target variable (the median value) as pandas dataframes. This\n",
    "# will make saving the data to a file a little easier later on.\n",
    "# shuffle the DataFrame rows \n",
    "df = df.sample(frac = 1, random_state = 10)\n",
    "\n",
    "X = df.drop(['conversion_flag'], axis=1)\n",
    "y = df['conversion_flag']\n",
    "\n",
    "# We split the dataset into 2/3 training and 1/3 testing sets.\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.25, random_state = 10)\n",
    "\n",
    "# Then we split the training set further into 2/3 training and 1/3 validation sets.\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_train, Y_train, test_size=0.33, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25444, 10)\n",
      "(12533, 10)\n",
      "(12660, 10)\n",
      "(25444,)\n",
      "(12533,)\n",
      "(12660,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4) Create Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll utilize Scikit Learn Pipeline to apply necessary preprocessing steps to the data. The following steps will be applied to the data:\n",
    "- Impute the median for null numeric features (income and age in our case)\n",
    "- Apply StandardScaler to numeric features\n",
    "- Impute 'missing' for null categorical data\n",
    "- onehot encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25444, 14)\n",
      "(12533, 14)\n",
      "(12660, 14)\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the data\n",
    "X_train_processed = pipeline.fit_transform(X_train)\n",
    "X_val_processed = pipeline.fit_transform(X_val)\n",
    "X_test_processed = pipeline.fit_transform(X_test)   \n",
    "\n",
    "# convert to pandas dataframe for convenience of writing to csv. Note that this step would not be ideal if the data\n",
    "# was very large\n",
    "X_train_processed = pd.DataFrame(X_train_processed, index=X_train.index)\n",
    "X_val_processed = pd.DataFrame(X_val_processed, index=X_val.index)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, index=X_test.index)\n",
    "\n",
    "print(X_train_processed.shape)\n",
    "print(X_val_processed.shape)\n",
    "print(X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5): Uploading the training and validation files to S3\n",
    "\n",
    "When a training job is constructed using SageMaker, a container is executed which performs the training operation. This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. We can use the SageMaker API to do this and hide some of the details.\n",
    "\n",
    "### Save the data locally\n",
    "\n",
    "First we need to create the train and validation csv files which we will then upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our local data directory. We need to make sure that it exists.\n",
    "data_dir = '../input'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use pandas to save our train and validation data to csv files. Note that we make sure not to include header\n",
    "# information or an index as this is required by the built in algorithms provided by Amazon. Also, it is assumed\n",
    "# that the first entry in each row is the target variable.\n",
    "X_test_processed.to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "X_test.to_csv(os.path.join(data_dir, 'test_unprocessed.csv'), header=True, index=False)\n",
    "Y_test.to_csv(os.path.join(data_dir, 'Y_test.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([Y_val, X_val_processed], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([Y_train, X_train_processed], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = session.default_bucket()\n",
    "prefix = 'mle-capstone'\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), bucket = bucket, key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), bucket = bucket, key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), bucket = bucket, key_prefix=prefix)\n",
    "Y_test_location = session.upload_data(os.path.join(data_dir, 'Y_test.csv'), bucket = bucket, key_prefix=prefix)\n",
    "test_unprocessed_location = session.upload_data(os.path.join(data_dir, 'test_unprocessed.csv'), bucket = bucket, key_prefix=prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
